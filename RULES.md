## Objective:

To create impactful proof-of-concept reasoning datasets and share them with the community. 

#### Submit Dataset:

Size: 
- Include at least 100 examples/rows to demonstrate your dataset's concept and quality

Documentation: 
- Provide a comprehensive dataset card that includes:
  - Clear description of the dataset's purpose and scope
  - Detailed explanation of how the dataset was created
  - Examples of how the dataset can be used for model training or evaluation
  - Any limitations or biases to be aware of

Accessibility: 
- Ensure your dataset has a valid dataset viewer preview for easy browsing

Discoverability: 
- Tag your dataset with `reasoning-datasets-competition` to be officially considered

Licensing: 
- Include clear licensing information that allows for research use

### Evaluation Criteria:

Approach, Domain, and Quality are the top criteria.

#### Domains:

- To expand reasoning beyond traditional STEM fields.

- **Legal reasoning:** Cases requiring application of statutes and precedents to reach judgments
- **Financial analysis:** Scenarios requiring evaluation of complex investment opportunities
- **Literary interpretation:** Texts requiring evidence-based analysis of themes and symbolism‚Äç
- **Ethics and philosophy:** Problems requiring structured moral or philosophical reasoning

#### Approach:

- Reasoning Datasets for Novel Tasks

While most reasoning datasets focus on improving benchmarks for mathematics or coding, there are other tasks where reasoning models could significantly improve performance:

- **Structured data extraction:** Datasets teaching models to extract and organize information from unstructured text (example approach)
- **Zero-shot classification:** Datasets focused on training smaller models to be more effective zero-shot classifiers through reasoning
- **Search improvement:** Reasoning datasets designed to enhance search relevance and accuracy
- **Diagrammatic reasoning:** Datasets that train models to interpret, analyze, and reason about visual representations like flowcharts, system diagrams, or decision trees
- **Constraint satisfaction problems:** Collections teaching models to reason through complex scheduling, resource allocation, or optimization scenarios with multiple interdependent constraints
- **Evidence evaluation:** Datasets demonstrating how to assess source credibility and weigh conflicting information‚Äç
- **Counterfactual reasoning:** Collections developing "what if" thinking by systematically altering variables and exploring potential outcomes

#### Competition Timeline üìÖ

- Launch Date: April 9, 2025
- Submission Deadline: May 1, 2025, at 11:59 pm Pacific Time
- Winners Announced: May 5, 2025
